---
title: 我不是药神_爬虫与分析
date: 2018-07-13 23:09:04
tags: 案例
---

《我不是药神》是由文牧野执导，徐峥、王传君、周一围、谭卓、章宇、杨新鸣等主演的喜剧电影，于 2018 年 7 月 6 日在中国上映。上映之后获得一片好评，不少观众甚至直呼“中国电影希望”，“《熔炉》、《辩护人》之类写实影片同水准”，诚然相较于市面上一众的抠图贴脸影视作品，《药神》在影片质量上确实好的多，不过我个人觉得《药神》的火爆还有以下几个原因：
* 影片题材稀少带来的新鲜感，像这类"针砭时弊" 类影视作品，国内太少
* 顺应潮流，目前《手机》事件及其带来的影响和国家层面文化自信的号召以及影视作品水平亟待提高的大环境下，《药神》的过审与上映本身也是对该类题材一定程度的鼓励
* 演员靠谱、演技扎实，这个没的说，特别是王传君的表现，让人眼前一亮  

本文通过爬取豆瓣电影评论，对该影片进行分析(ps:千古代码一大抄，网上的各种参考代码，各种不靠谱，最后还是自己一个个坑去解决)
<!--more-->
截止7月13日凌晨：豆瓣评分：8.9 分，猫眼：9.7 分，时光网：8.8 分 。豆瓣的评分质量相对而言要靠谱点，所以本文数据来源也是豆瓣。

## 0需求分析

## 1前期准备

### 1.1网页分析
豆瓣从2017.10月开始全面禁止爬取数据，仅仅开放500条数据，白天1分钟最多可以爬取40次，晚上一分钟可爬取60次数，超过此次数则会封禁IP地址1天  
> tips发现

实际操作发现，点击影片评论页面的`后页`时，url中的一个参数`start`会加20，但是如果直接赋予'start'每次增加10，网页也是可以存在的！
* 登录状态下，按网页按钮点击`后页`，`start`最多为480，也就是20*25=500条
* 非登录状态下，最多为200条
* 登录状态下，直接修改url的方法可以比官方放出的评论数量多出了一倍！！！

### 1.2页面布局分析
本次使用xpath解析，因为之前的博客案例用过正则，也用过beautifulsoup，这次尝试不一样的方法
如下图所示，本此数据爬取主要获取的内容有
* 评论用户ID
* 评论内容
* 评分
* 评论日期
* 用户所在城市
![](页面布局.png)
> 城市信息获取

评论页面没有城市信息，因此需要通过进入评论用户主页去获取城市信息元素
![](城市信息.png)
通过分析页面发下，用户ID名称里隐藏着主页链接！
所以我的思路就是request该链接，然后提取城市信息
![](用户城市.png)

## 2数据获取-爬虫
### 2.1获取cookies
因为豆瓣的爬虫限制，所以需要使用cookies作身份验证，通过chrome获取cooikes如下
![](cookies.png)

### 2.2加载cookies与headers
``` python
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36'}
    cookies = {'cookie': 'bid=GOOb4vXwNcc; douban-fav-remind=1; viewed="27611266_26886337"; ps=y; ue="maplekonghou@163.com"; " \
                   "push_noty_num=0; push_doumail_num=0; ap=1; loc-last-index-location-id="108288"; ll="108288"; dbcl2="181095881:N/y1wyPpmA8"; ck=1wlL'}
    url = "https://movie.douban.com/subject/" + str(id) + "/comments?start=" + str(page * 10) + "&limit=20&sort=new_score&status=P"
    res = requests.get(url, headers=headers, cookies=cookies)
    res.encoding = "utf-8"
    if (res.status_code == 200):
        print("\n第{}页短评爬取成功！".format(page + 1))
        print(url)
    else:
        print("\n第{}页爬取失败！".format(page + 1))
```
一般刷新页面后，第一个请求里包含了cookies
### 2.3延时反爬虫
设置延时发出请求，并且延时的值还保留了2位小数（自我感觉模拟正常访问请求会更加逼真...待证实）
``` python 
        time.sleep(round(random.uniform(1, 2), 2))
``` 

### 2.4解析需求数据
这里有个大bug，找了好久！  
因为有的用户虽然给了评论，但是没有给评分，所以score和date这两个的xpath位置是会变动的。。。
所以需要加判断，如果发现score里面解析的是日期，证明该条评论没有给出评分
``` python
    x = etree.HTML(res.text)
    for i in range(1, 21):
        name = x.xpath('//*[@id="comments"]/div[{}]/div[2]/h3/span[2]/a/text()'.format(i))
        # 下面是个大bug，如果有的人没有评分，但是评论了，那么score解析出来是日期，而日期所在位置spen[3]为空
        score = x.xpath('//*[@id="comments"]/div[{}]/div[2]/h3/span[2]/span[2]/@title'.format(i))
        date = x.xpath('//*[@id="comments"]/div[{}]/div[2]/h3/span[2]/span[3]/@title'.format(i))
        m = '\d{4}-\d{2}-\d{2}'
        match = re.compile(m).match(score[0])
        if match is not None:
            date = score
            score = ["null"]
        else:
            pass
        content = x.xpath('//*[@id="comments"]/div[{}]/div[2]/p/span/text()'.format(i))
        id = x.xpath('//*[@id="comments"]/div[{}]/div[2]/h3/span[2]/a/@href'.format(i))
        try:
            city = get_city(id[0], i)  # 调用评论用户的ID城市信息获取
        except IndexError:
            city = " "
        name_list.append(str(name[0]))
        score_list.append(str(score[0]).strip('[]\''))  # bug 有些人评论了文字，但是没有给出评分
        date_list.append(str(date[0]).strip('[\'').split(' ')[0])
        content_list.append(str(content[0]).strip())
        city_list.append(city)
``` 
## 3数据存储
因为数据量不是很大，因为普通csv存储足够，把获取的数据转换为pandas的DF格式，然后存储到csv文件中
``` python
    infos = {'name': name_list, 'city': city_list, 'content': content_list, 'score': score_list, 'date': date_list}
    data = pd.DataFrame(infos, columns=['name', 'city', 'content', 'score', 'date'])
    data.to_csv(str(ID) + "_comments.csv")

```
因为考虑到代码的复用性，所以main函数中传入了两个参数，
* 一个是待分析影片在豆瓣电影中的ID号(这个可以在链接中获取到，是一个8位数
* 一个是需要爬取的页码数，一般设置为49，因为网站只开放500条评论
另外有些电影评论有可能不足500条，所以需要调整，之前尝试通过正则匹配分析页面结构
![](末尾页.png)
来判断是否是评论的末尾页，但是没成功..有兴趣的读者可以尝试

## 4数据清洗
爬取出来的结果如下
![](爬取结果.png)
分别为：评论序列号，用户名，用户所在城市，评论内容， 评分， 评论日期

### 4.1城市信息清洗
从爬取的结果分析可以发现，城市信息数据有以下问题
* 有的城市空缺
* 海外城市
* 乱写
* pyecharts不支持的城市

> step1:过滤筛选中文字

通过正则表达式筛选中文，通过split函数提取清理中文，通过sub函数替换掉标点符号
``` python
def translate(str):
    line = str.strip()
    p2 = re.compile('[^\u4e00-\u9fa5]')  # 中文的编码范围是：\u4e00到\u9fa5
    zh = " ".join(p2.split(line)).strip()
    zh = ",".join(zh.split())
    str = re.sub("[A-Za-z0-9!！，%\[\],。]", "", zh)
    return str
``` 
> step2:匹配pyecharts支持的城市列表

一开始我不知道该库有城市列表资料（只找了官网，没看github）所以使用的方法如下，自己上网找中国城市字典，然后用excel 筛选和列表分割功能快速获得一个不包含省份和'市'的城市字典，然后匹配。后来去github上issue了下，发现有现成的字典文件，一个json文本，得到的回复如下(*^__^*) 
![](城市列表)
这样就方便了，直接和这个列表匹配就完了，不在里面的话，直接list.pop就可以了
 但是这样还有个问题，就是爬取下来的城市信息中还包含着省份，而pyecharts中是不能带省份的，所以还需要通过分割，来提取城市，可能存在的情况有
 *  两个字：北京
 *  三个字：攀枝花
 *  四个字：山东烟台
 *  五个字：四川攀枝花
 *  六个字：黑龙江哈尔滨
 *  。。。
 
 
 因此我做了简化处理：
 * 名称为2~4的，如果没匹配到，则提取后2个字，作为城市名
 * 名称为>4的，如果没匹配到，则依次尝试提取后面5、4、3个字的
 * 其余情况，经过观察原始数据发现数量极其稀少，可以忽略不作处理
``` python
 def count_city(csv_file):
    citys= []
    d = pd.read_csv(csv_file)
    for i in d['city'].dropna():       # 过滤掉空的城市
        i = translate(i)  # 只保留中文
        if len(i)>1 and len(i)<5:  # 如果名称个数2~4，先判断是否在字典里
            if i in fth:
                citys.append(i)
            else:
                i = i[-2:]  # 取城市名称后两个，去掉省份
                if i in fth:
                    citys.append(i)
                else:
                    pass
        if len(i) > 4:
            if i in fth:   # 如果名称个数>2，先判断是否在字典里
                citys.append(i)
            if i[-5:] in fth:
                citys.append(i[-4:])
                continue
            if i[-4:] in fth:
                citys.append(i[-4:])
                continue
            if i[-3:] in fth:
                citys.append(i[-3:])
            else:
                pass
    result = {}
    while '' in citys:
        citys.remove('')  # 去掉字符串中的空值
    print("城市总数量为：",len(citys))
    for i in set(citys):
        result[i] = citys.count(i)
    return result
``` 
 但是这样可能还有漏洞，所以为保证程序一定不出错，又设计了如下校验模块：
 思路就是，循环尝试，根据`xx.add()`函数的报错，确定城市名不匹配，然后从`list`中把错误城市`pop`掉，另外注意到豆瓣个人主页上的城市信息一般都是是到市，那么县一级的区域就不考虑了，这也算是一种简化处理
``` python
    while True:   # 二次筛选，和pyecharts支持的城市库进行匹配，如果报错则删除该城市对应的统计
        try:
            attr, val = geo.cast(info)
            geo.add("", attr, val, visual_range=[0, 300], visual_text_color="#fff", is_geo_effect_show=False,
                    is_piecewise=True, visual_split_number=6, symbol_size=15, is_visualmap=True)
            flag = 1
        except ValueError as e:
            e = str(e)
            e = e.split("No coordinate is specified for ")[1]  # 获取不支持的城市名称
            info.pop(e)
        if flag == 1:
            break
``` 
## 5基于snownlp的情感分析
snownlp主要可以进行中文分词（算法是Character-Based Generative Model）、词性标注（原理是TnT、3-gram 隐马）、情感分析（官网木有介绍原理，但是指明购物类的评论的准确率较高，其实是因为它的语料库主要是购物方面的，可以自己构建相关领域语料库，替换原来的，准确率也挺不错的）、文本分类（原理是朴素贝叶斯）、转换拼音、繁体转简体、提取文本关键词（原理是TextRank）、提取摘要（原理是TextRank）、分割句子、文本相似（原理是BM25）。
官网还有更多关于该库的介绍，在看我这个文章之前，建议先看一下官网，里面有最基础的一些命令的介绍。官网[链接](https://pypi.python.org/pypi/snownlp/)

由于snownlp全部是unicode编码，所以要注意数据是否为unicode编码。因为是unicode编码，所以不需要去除中文文本里面含有的英文，因为都会被转码成统一的编码
上面只是调用snownlp原生语料库对文本进行分析，snownlp重点针对购物评价领域，所以为了提高情感分析的准确度可以采取以下措施
### 5.1标记评论样本数据
* 取出一部分的评论，人工标记值为+1、-1分别代表褒义和贬义
* 训练语料库
``` python
from snownlp import sentiment #加载情感分析模块
sentiment.train('E:/Anaconda2/Lib/site-packages/snownlp/sentiment/neg.txt', 'E:/Anaconda2/Lib/site-packages/snownlp/sentiment/pos.txt') #对语料库进行训练，把路径改成相应的位置。我这次练习并没有构建语料库，用了默认的，所以把路径写到了sentiment模块下。
sentiment.save('D:/pyscript/sentiment.marshal') #这一步是对上一步的训练结果进行保存，如果以后语料库没有改变，下次不用再进行训练，直接使用就可以了，所以一定要保存，保存位置可以自己决定，但是要把`snownlp/seg/__init__.py`里的`data_path`也改成你保存的位置，不然下次使用还是默认的。

``` 
* 进行预测
``` python
from snownlp import SnowNLP
senti=[SnowNLP(i).sentiments for i in text1] #遍历每条评论进行预测
``` 
* 验证准确率
预测结果为positive的概率，positive的概率大于等于0.6，我认为可以判断为积极情感，小于0.6的判断为消极情感。所以以下将概率大于等于0.6的评论标签赋为1，小于0.6的评论标签赋为-1，方便后面与实际标签进行比较。
``` python
newsenti=[]
for i in senti:
  if (i>=0.6):
      newsenti.append(1)
  else:
      newsenti.append(-1)
text['predict']=newsenti  # 将新的预测标签增加为text的某一列，所以现在text的第0列为评论文本，第1列为实际标签，第2列为预测标签

counts=0
for j in range(len(text.iloc[:,0])): # 遍历所有标签，将预测标签和实际标签进行比较，相同则判断正确。
    if text.iloc[j,2]==text.iloc[j,1]:
        counts+=1
print(u"准确率为:%f"%(float(counts)/float(len(text)))) # 输出本次预测的准确率
``` 
* 总结
只是练习一下，所以没有自己构建该领域的语料库，如果构建了相关语料库，替换默认语料库，准确率会高很多。所以语料库是非常关键的，如果要正式进行文本挖掘，建议要构建自己的语料库。









