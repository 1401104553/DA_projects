---
title: 我不是药神_爬虫与分析
date: 2018-07-13 23:09:04
tags: 案例
---

《我不是药神》是由文牧野执导，徐峥、王传君、周一围、谭卓、章宇、杨新鸣等主演的喜剧电影，于 2018 年 7 月 6 日在中国上映。上映之后获得一片好评，不少观众甚至直呼“中国电影希望”，“《熔炉》、《辩护人》之类写实影片同水准”，诚然相较于市面上一众的抠图贴脸影视作品，《药神》在影片质量上确实好的多，不过我个人觉得《药神》的火爆还有以下几个原因：
* 影片题材稀少带来的新鲜感，像这类"针砭时弊" 类影视作品，国内太少
* 顺应潮流，目前《手机》事件及其带来的影响和国家层面文化自信的号召以及影视作品水平亟待提高的大环境下，《药神》的过审与上映本身也是对该类题材一定程度的鼓励
* 演员靠谱、演技扎实，这个没的说，特别是王传君的表现，让人眼前一亮  

本文通过爬取豆瓣电影评论，对该影片进行分析(ps:千古代码一大抄，网上的各种参考代码，各种不靠谱，最后还是自己一个个坑去解决)
<!--more-->
截止7月13日凌晨：豆瓣评分：8.9 分，猫眼：9.7 分，时光网：8.8 分 。豆瓣的评分质量相对而言要靠谱点，所以本文数据来源也是豆瓣。

## 0需求分析

## 1前期准备

### 1.1网页分析
豆瓣从2017.10月开始全面禁止爬取数据，仅仅开放500条数据，白天1分钟最多可以爬取40次，晚上一分钟可爬取60次数，超过此次数则会封禁IP地址1天  
> tips发现

实际操作发现，点击影片评论页面的`后页`时，url中的一个参数`start`会加20，但是如果直接赋予'start'每次增加10，网页也是可以存在的！
* 登录状态下，按网页按钮点击`后页`，`start`最多为480，也就是20*25=500条
* 非登录状态下，最多为200条
* 登录状态下，直接修改url的方法可以比官方放出的评论数量多出了一倍！！！

### 1.2页面布局分析
本次使用xpath解析，因为之前的博客案例用过正则，也用过beautifulsoup，这次尝试不一样的方法
如下图所示，本此数据爬取主要获取的内容有
* 评论用户ID
* 评论内容
* 评分
* 评论日期
* 用户所在城市
![](页面布局.png)
> 城市信息获取

评论页面没有城市信息，因此需要通过进入评论用户主页去获取城市信息元素
![](城市信息.png)
通过分析页面发下，用户ID名称里隐藏着主页链接！
所以我的思路就是request该链接，然后提取城市信息
![](用户城市.png)

## 2数据获取-爬虫
### 2.1获取cookies
因为豆瓣的爬虫限制，所以需要使用cookies作身份验证，通过chrome获取cooikes如下
![](cookies.png)

### 2.2加载cookies与headers
``` python
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36'}
    cookies = {'cookie': 'bid=GOOb4vXwNcc; douban-fav-remind=1; viewed="27611266_26886337"; ps=y; ue="maplekonghou@163.com"; " \
                   "push_noty_num=0; push_doumail_num=0; ap=1; loc-last-index-location-id="108288"; ll="108288"; dbcl2="181095881:N/y1wyPpmA8"; ck=1wlL'}
    url = "https://movie.douban.com/subject/" + str(id) + "/comments?start=" + str(page * 10) + "&limit=20&sort=new_score&status=P"
    res = requests.get(url, headers=headers, cookies=cookies)
    res.encoding = "utf-8"
    if (res.status_code == 200):
        print("\n第{}页短评爬取成功！".format(page + 1))
        print(url)
    else:
        print("\n第{}页爬取失败！".format(page + 1))
```
一般刷新页面后，第一个请求里包含了cookies
### 2.3延时反爬虫
设置延时发出请求，并且延时的值还保留了2位小数（自我感觉模拟正常访问请求会更加逼真...待证实）
``` python 
        time.sleep(round(random.uniform(1, 2), 2))
``` 

### 2.4解析需求数据
这里有个大bug，找了好久！  
因为有的用户虽然给了评论，但是没有给评分，所以score和date这两个的xpath位置是会变动的。。。
所以需要加判断，如果发现score里面解析的是日期，证明该条评论没有给出评分
``` python
    x = etree.HTML(res.text)
    for i in range(1, 21):
        name = x.xpath('//*[@id="comments"]/div[{}]/div[2]/h3/span[2]/a/text()'.format(i))
        # 下面是个大bug，如果有的人没有评分，但是评论了，那么score解析出来是日期，而日期所在位置spen[3]为空
        score = x.xpath('//*[@id="comments"]/div[{}]/div[2]/h3/span[2]/span[2]/@title'.format(i))
        date = x.xpath('//*[@id="comments"]/div[{}]/div[2]/h3/span[2]/span[3]/@title'.format(i))
        m = '\d{4}-\d{2}-\d{2}'
        match = re.compile(m).match(score[0])
        if match is not None:
            date = score
            score = ["null"]
        else:
            pass
        content = x.xpath('//*[@id="comments"]/div[{}]/div[2]/p/span/text()'.format(i))
        id = x.xpath('//*[@id="comments"]/div[{}]/div[2]/h3/span[2]/a/@href'.format(i))
        try:
            city = get_city(id[0], i)  # 调用评论用户的ID城市信息获取
        except IndexError:
            city = " "
        name_list.append(str(name[0]))
        score_list.append(str(score[0]).strip('[]\''))  # bug 有些人评论了文字，但是没有给出评分
        date_list.append(str(date[0]).strip('[\'').split(' ')[0])
        content_list.append(str(content[0]).strip())
        city_list.append(city)
``` 
## 3数据存储
因为数据量不是很大，因为普通csv存储足够，把获取的数据转换为pandas的DF格式，然后存储到csv文件中
``` python
    infos = {'name': name_list, 'city': city_list, 'content': content_list, 'score': score_list, 'date': date_list}
    data = pd.DataFrame(infos, columns=['name', 'city', 'content', 'score', 'date'])
    data.to_csv(str(ID) + "_comments.csv")

```
因为考虑到代码的复用性，所以main函数中传入了两个参数，
* 一个是待分析影片在豆瓣电影中的ID号(这个可以在链接中获取到，是一个8位数
* 一个是需要爬取的页码数，一般设置为49，因为网站只开放500条评论
另外有些电影评论有可能不足500条，所以需要调整，之前尝试通过正则匹配分析页面结构
![](末尾页.png)
来判断是否是评论的末尾页，但是没成功..有兴趣的读者可以尝试

## 4数据清洗
爬取出来的结果如下
![](爬取结果.png)
分别为：评论序列号，用户名，用户所在城市，评论内容， 评分， 评论日期



## 5基于snownlp的情感分析
snownlp主要可以进行中文分词（算法是Character-Based Generative Model）、词性标注（原理是TnT、3-gram 隐马）、情感分析（官网木有介绍原理，但是指明购物类的评论的准确率较高，其实是因为它的语料库主要是购物方面的，可以自己构建相关领域语料库，替换原来的，准确率也挺不错的）、文本分类（原理是朴素贝叶斯）、转换拼音、繁体转简体、提取文本关键词（原理是TextRank）、提取摘要（原理是TextRank）、分割句子、文本相似（原理是BM25）。
官网还有更多关于该库的介绍，在看我这个文章之前，建议先看一下官网，里面有最基础的一些命令的介绍。官网[链接](https://pypi.python.org/pypi/snownlp/)

由于snownlp全部是unicode编码，所以要注意数据是否为unicode编码。因为是unicode编码，所以不需要去除中文文本里面含有的英文，因为都会被转码成统一的编码

### 5.1 城市信息清洗


上面只是调用snownlp原生语料库对文本进行分析，snownlp重点针对购物评价领域，所以为了提高情感分析的准确度可以采取以下措施
### 5.1标记评论样本数据
* 取出一部分的评论，人工标记值为+1、-1分别代表褒义和贬义
* 训练语料库
``` python
from snownlp import sentiment #加载情感分析模块
sentiment.train('E:/Anaconda2/Lib/site-packages/snownlp/sentiment/neg.txt', 'E:/Anaconda2/Lib/site-packages/snownlp/sentiment/pos.txt') #对语料库进行训练，把路径改成相应的位置。我这次练习并没有构建语料库，用了默认的，所以把路径写到了sentiment模块下。
sentiment.save('D:/pyscript/sentiment.marshal') #这一步是对上一步的训练结果进行保存，如果以后语料库没有改变，下次不用再进行训练，直接使用就可以了，所以一定要保存，保存位置可以自己决定，但是要把`snownlp/seg/__init__.py`里的`data_path`也改成你保存的位置，不然下次使用还是默认的。

``` 
* 进行预测
``` python
from snownlp import SnowNLP
senti=[SnowNLP(i).sentiments for i in text1] #遍历每条评论进行预测
``` 
* 验证准确率
预测结果为positive的概率，positive的概率大于等于0.6，我认为可以判断为积极情感，小于0.6的判断为消极情感。所以以下将概率大于等于0.6的评论标签赋为1，小于0.6的评论标签赋为-1，方便后面与实际标签进行比较。
``` python
newsenti=[]
for i in senti:
  if (i>=0.6):
      newsenti.append(1)
  else:
      newsenti.append(-1)
text['predict']=newsenti  # 将新的预测标签增加为text的某一列，所以现在text的第0列为评论文本，第1列为实际标签，第2列为预测标签

counts=0
for j in range(len(text.iloc[:,0])): # 遍历所有标签，将预测标签和实际标签进行比较，相同则判断正确。
    if text.iloc[j,2]==text.iloc[j,1]:
        counts+=1
print(u"准确率为:%f"%(float(counts)/float(len(text)))) # 输出本次预测的准确率
``` 
* 总结
只是练习一下，所以没有自己构建该领域的语料库，如果构建了相关语料库，替换默认语料库，准确率会高很多。所以语料库是非常关键的，如果要正式进行文本挖掘，建议要构建自己的语料库。









